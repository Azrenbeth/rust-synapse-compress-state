# Compress Synapse State Tables

An experimental tool that reads in the rows from `state_groups_state` and
`state_group_edges` tables for a particular room and calculates the changes that
could be made that (hopefully) will significantly reduce the number of rows.

This tool currently *does not* write to the database in any way, so should be
safe to run. If the `-o` option is specified then SQL will be written to the
given file that would change the tables to match the calculated state. (Note
that if `-t` is given then each change to a particular state group is wrapped
in a transaction).

The SQL generated by the `-o` option is safe to apply against the database with
Synapse running. This is because the `state_groups` and `state_groups_state`
tables are append-only: once written to the database, they are never modified.
There is therefore no danger of a modification racing against a running synapse.
Further, this script makes its changes within atomic transactions, and each
transaction should not affect the results from any of the queries that synapse
performs.

The tool will also ensure that the generated state deltas do give the same state
as the existing state deltas before generating any SQL.

## Introduction to the state tables
### What is state?
State is things like who is in a room, what the room topic/name is, who has
what priviledge levels etc. Synapse keeps track of it so that it can spot invalid
events (e.g. ones sent by banned users, or by people with insufficient priviledge).

### What is a state group?

Synapse needs to keep track of the state at the moment of each event. A state group
corresponds to a unique state. The database table `event_to_state_groups` keeps track
of the mapping from event ids to state group ids.

Consider the following simplified example:
```
State group id   |          State
_____________________________________________
       1         |      Alice in room
       2         | Alice in room, Bob in room
       3         |        Bob in room


Event id |     What the event was
______________________________________
    1    |    Alice sends a message
    3    |     Bob joins the room
    4    |     Bob sends a message
    5    |    Alice leaves the room
    6    |     Bob sends a message


Event id | State group id
_________________________
    1    |       1
    2    |       1
    3    |       2
    4    |       2
    5    |       3
    6    |       3
```
### What are deltas and predecessors?
When a new state event happens (e.g. Bob joins the room) a new state group is created.
BUT instead of copying all of the state from the previous state group, we just store
the change from the previous group (saving on lots of storage space!). The difference
from the previous state group is called the "delta"

So for the previous example we would have the following (Note only rows 1 and two will
make sense at this point):

```
State group id | Previous state group id |      Delta
____________________________________________________________
       1       |          NONE           |   Alice in room
       2       |           1             |    Bob in room
       3       |          NONE           |    Bob in room
```
So what happened with row 3? Well the way that deltas work in synapse is that they
can only add new state or overwrite old state, but they cannot remove it. (So if the 
room topic is changed then that is just overwriting state, but removing alice from
the room is neither an addition or an overwriting). If it is impossible to find
a delta, then you just start from scratch again with a "snapshot" of the entire state.

(NOTE this might not be how synapse handles leaving rooms but it works for illustrative
purposes)

The state of a state group is worked out by following the previous state group's and adding
together all of the deltas (with the most recent taking precedence)

The mapping from state group to previous state group takes place in `state_group_edges` 
and the deltas are stored in `state_groups_state`

### What are we compressing then?
In order to speed up the converstion from state group id to state, there is a limit of 100 
hops set by synapse (that is we will only ever have to lookup the deltas for a maximum of 
100 state groups). It does this by taking another "snapshot" every 100 state groups.

However, it is these snapshots that take up the bulk of the storage in a synapse database,
so we want to find a way to reduce the number of them without dramatically increasing the 
maximum number of hops needed to do lookups.

## Algorithm

The algorithm works by attempting to create a tree of deltas, produced by
appending state groups to different "levels". Each level has a maximum size, where
each state group is appended to the lowest level that is not full.

This produces a graph that looks approximately like the following, in the case
of having two levels with the bottom level (L1) having a maximum size of 3:

```
L2 <-------------------- L2 <---------- ...
^--- L1 <--- L1 <--- L1  ^--- L1 <--- L1 <--- L1
```

The sizes and number of levels used can be controlled via `-l`, and defaults to 3
levels of sizes 100, 50 and 25.

**Note**: Increasing the sum of the sizes of levels will increase the time it
takes for to query the full state of a given state group. By default Synapse
attempts to keep this below 100.


## Example usage

```
$ synapse_compress_state -p "postgresql://localhost/synapse" -r '!some_room:example.com' -o out.sql -t
Fetching state from DB for room '!some_room:example.com'...
Got initial state from database. Checking for any missing state groups...
Number of state groups: 73904
Number of rows in current table: 2240043
Number of rows after compression: 165754 (7.40%)
Compression Statistics:
  Number of forced resets due to lacking prev: 34
  Number of compressed rows caused by the above: 17092
  Number of state groups changed: 2748
New state map matches old one

# It's finished, so we can now go and rewrite the DB
$ psql synapse < out.data
```

## Running Options

- -s [MIN_STATE_GROUP]  
The state group to start processing from (non inclusive)

- -n [GROUPS_TO_COMPRESS]  
How many groups to load into memory to compress (starting
from the 1st group in the room or the group specified by -s)

- -l [LEVELS]  
Sizes of each new level in the compression algorithm, as a comma separated list.
The first entry in the list is for the lowest, most granular level, with each 
subsequent entry being for the next highest level. The number of entries in the
list determines the number of levels that will be used. The sum of the sizes of
the levels effect the performance of fetching the state from the database, as the
sum of the sizes is the upper bound on number of iterations needed to fetch a
 given set of state. [default's to 100,50,25]

- -m [COUNT]  
If the compressor cannot save this many rows from the database then it will stop early

- -o [FILE]  
File to output the SQL transactions to (for later running on the database)

- -p [URL] **Required**  
The url for connecting to the postgres database. This should be of the form
"postgresql://username:password@mydomain.com/database"

- -r [ROOM_ID] **Required**  
The room to process (this is the value found in the `rooms` table of the database
not the common name for the room - is should look like: "!wOlkWNmgkAZFxbTaqj:matrix.org"

- -t  
If this flag is set then then each change to a particular state group is wrapped in a transaction. This should be done if you wish to apply the changes while synapse is still running.

- -g  
If this flag is set then output the node and edge information for the state_group
directed graph built up from the predecessor state_group links. These can be looked
at in something like Gephi (https://gephi.org)

- -c  
If this flag is set then the changes the compressor makes will be committed to the
database. This should be safe to use while synapse is running as it assumes by default
that the transactions flag is set

## Using as python library

The compressor can also be built into a python library as it uses PyO3. It can be
built and installed into the current virtual environment by running `maturin develop`

All the same running options are available, see the comments in the Config struct
in lib.rs for the names of each argument (N.B. Python expects them all to be 
passed as strings - except for `graphs` which should be `True` or `False`). All
arguments other than `db_url` and `room_id` are optional.

The following code does exactly the same as the command-line example from above:
```
import synapse_compress_state as comp

comp.run_compression(
  db_url="postgresql://localhost/synapse",
  room_id="!some_room:example.com",
  output_file="out.sql",
  transactions=True
)
```

Note: since this library uses Jemalloc, you might get an error of the form:
```
ImportError: /[LONG_PATH]/synapse_compress_state.abi3.so: cannot allocate memory in static TLS block
```
If this happens then try running the following:
```
LD_PATH=/[LONG_PATH]/synapse_compress_state.abi3.so ./my_python_script
```

## Running tests

To run the integration tests, you first need to start up a postgres database
for the libary to talk to. There is a docker-compose file that sets one up
with all of the correct tables. The tests can therefore be run as follows:

```
docker-compose up -d
cargo test
docker-compose down
```
